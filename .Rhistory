# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=2:4)
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
#library(cluster)  # clustering algorithms
#library(factoextra) # visualization
#library(purrr) # to use map_dbl() function
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=2:4)
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=2:4)
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "single")
Plot the obtained dendrogram
Plot the obtained dendrogram
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "single")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=2:4)
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=2:4)
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=6:10)
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=80:100)
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=800:1000)
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang=-1)
sub_grps<- cutree(hc1, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc1, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc1, k = 3, border=2:4)
# AIM:- TO PERFORM THE CLASSIFICATION BY BAYESIAN CLASSIFICATION ALGORITHM USING R.
library(e1071) # Provides functions for statistic and probabilistic algorithms
library(caTools)
library(caret)  # ML Library in R enables to train different types of algorithms using a simple train function.
data("iris")
str(iris)
summary(iris)
# Splitting data into train and test data
split = sample.split(iris, SplitRatio = 0.7)
dataTrain = subset(iris, split==TRUE)
dataTest = subset(iris, split==FALSE)
dataTrain
dataTest
# Feature Scaling
train_scale <- scale(dataTrain[, 1:4])
test_scale <- scale(dataTest[, 1:4])
# Fitting Naive Bayes Model to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,dataTrain)
classifier_cl
# Predicting on test data'
dataTestPred <- predict(classifier_cl,dataTest)
dataTestPred
# caTools Contains basic utility functions:fast calculation of AUC, LogitBoost classifier, round-off-error-free sum and cumsum (cumulative sum).
#A toolkit with infrastructure for representing, summarizing, and visualizing tree-structured regression and classification models.
# AIM:- TO PERFORM THE CLASSIFICATION BY DECISION TREE USING R PROGRAMMING.
library(RWeka)
library(partykit)
library(caTools)
data("iris")
View(iris)
summary(iris)
#splitting
split = sample.split(iris, SplitRatio = 0.7)
dataTrain = subset(iris, split==TRUE)
dataTest = subset(iris, split==FALSE)
# model
m1 <- J48(Species~., dataTrain)
summary(m1)
# Prediction
dataTestPred <- predict(m1, dataTest)
table_matrix <- table(dataTest$Species, dataTestPred)
print(table_matrix)
accuracy_Test <- sum(diag(table_matrix)) / sum(table_matrix)
cat("Test Accuracy is: ", accuracy_Test)
# Iris_decision_plot
plot(m1, type="simple")
# AIM:- TO PERFORM THE CLASSIFICATION BY BAYESIAN CLASSIFICATION ALGORITHM USING R.
library(e1071) # Provides functions for statistic and probabilistic algorithms
library(caTools)
library(caret)  # ML Library in R enables to train different types of algorithms using a simple train function.
data("iris")
str(iris)
summary(iris)
# Splitting data into train and test data
split = sample.split(iris, SplitRatio = 0.7)
dataTrain = subset(iris, split==TRUE)
dataTest = subset(iris, split==FALSE)
dataTrain
dataTest
# Feature Scaling
train_scale <- scale(dataTrain[, 1:4])
test_scale <- scale(dataTest[, 1:4])
# Fitting Naive Bayes Model to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,dataTrain)
classifier_cl
# Predicting on test data'
dataTestPred <- predict(classifier_cl,dataTest)
dataTestPred
# Confusion Matrix
table_matrix <- table(dataTest$Species, dataTestPred)
print(table_matrix)
# Model Evaluation
confusionMatrix(table_matrix)
# AIM:- TO PERFORM THE CLASSIFICATION BY BAYESIAN CLASSIFICATION ALGORITHM USING R.
library(e1071) # Provides functions for statistic and probabilistic algorithms
library(caTools)
library(caret)  # ML Library in R enables to train different types of algorithms using a simple train function.
data("iris")
str(iris)
summary(iris)
# Splitting data into train and test data
split = sample.split(iris, SplitRatio = 0.7)
dataTrain = subset(iris, split==TRUE)
dataTest = subset(iris, split==FALSE)
dataTrain
dataTest
# Feature Scaling
train_scale <- scale(dataTrain[, 1:4])
test_scale <- scale(dataTest[, 1:4])
# Fitting Naive Bayes Model to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,dataTrain)
classifier_cl
# Predicting on test data'
dataTestPred <- predict(classifier_cl,dataTest)
dataTestPred
# Confusion Matrix
table_matrix <- table(dataTest$Species, dataTestPred)
print(table_matrix)
# Model Evaluation
confusionMatrix(table_matrix)
# AIM:- TO PERFORM CLUSTER ANALYSIS BY K-MEANS METHOD USING R.
str(iris)
# Loading package
library(ClusterR)
library(cluster)
library(caret)
# Removing initial label of Species from original dataset
iris_data <- iris[, -5]
iris_data
# Fitting K-Means clustering Model to training dataset
set.seed(123) # Setting seed
km <- kmeans(iris_data, centers = 3, nstart = 20)
km
#Cluster identification for each observation
km$cluster
# Model Evaluation and visualization
plot(iris_data[c("Sepal.Length", "Sepal.Width")])
plot(iris_data[c("Sepal.Length", "Sepal.Width")],col = km$cluster)
plot
## Plotting cluster centers
km$centers
km$centers[, c("Sepal.Length", "Sepal.Width")]
# cex is font size, pch is symbol
points(km$centers[, c("Sepal.Length", "Sepal.Width")],col = 1:3, pch = 8, cex = 3)
## Visualizing clusters
y_kmeans <- km$cluster
clusplot(iris_data[, c("Sepal.Length", "Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab = 'Sepal.Length',
ylab = 'Sepal.Width')
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc, cex = 0.6, hang=-1)
sub_grps<- cutree(hc, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc, k = 3, border=2:4)
# AIM:- TO PERFORM CLUSTER ANALYSIS BY K-MEANS METHOD USING R.
str(iris)
# Loading package
library(ClusterR)
library(cluster)
library(caret)
# Removing initial label of Species from original dataset
iris_data <- iris[, -5]
iris_data
# Fitting K-Means clustering Model to training dataset
set.seed(123) # Setting seed
km <- kmeans(iris_data, centers = 3, nstart = 20)
km
#Cluster identification for each observation
km$cluster
# Model Evaluation and visualization
plot(iris_data[c("Sepal.Length", "Sepal.Width")])
plot(iris_data[c("Sepal.Length", "Sepal.Width")],col = km$cluster)
plot
## Plotting cluster centers
km$centers
km$centers[, c("Sepal.Length", "Sepal.Width")]
# cex is font size, pch is symbol
points(km$centers[, c("Sepal.Length", "Sepal.Width")],col = 1:3, pch = 8, cex = 3)
## Visualizing clusters
y_kmeans <- km$cluster
clusplot(iris_data[, c("Sepal.Length", "Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab = 'Sepal.Length',
ylab = 'Sepal.Width')
# AIM:- TO PERFORM CLUSTER ANALYSIS BY K-MEANS METHOD USING R.
str(iris)
# Loading package
library(ClusterR)
library(cluster)
library(caret)
# Removing initial label of Species from original dataset
iris_data <- iris[, -5]
iris_data
# Fitting K-Means clustering Model to training dataset
set.seed(123) # Setting seed
km <- kmeans(iris_data, centers = 3, nstart = 20)
km
#Cluster identification for each observation
km$cluster
# Model Evaluation and visualization
plot(iris_data[c("Sepal.Length", "Sepal.Width")])
# AIM:- TO PERFORM CLUSTER ANALYSIS BY K-MEANS METHOD USING R.
str(iris)
# Loading package
library(ClusterR)
library(cluster)
library(caret)
# Removing initial label of Species from original dataset
iris_data <- iris[, -5]
iris_data
# Fitting K-Means clustering Model to training dataset
set.seed(123) # Setting seed
km <- kmeans(iris_data, centers = 3, nstart = 20)
km
#Cluster identification for each observation
km$cluster
# Model Evaluation and visualization
plot(iris_data[c("Sepal.Length", "Sepal.Width")])
plot(iris_data[c("Sepal.Length", "Sepal.Width")],col = km$cluster)
plot
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc, cex = 0.6, hang=-1)
sub_grps<- cutree(hc, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc, k = 3, border=2:4)
# AIM:-	To perform the hierarchical clustering using R programming.
# Loading package
library(cluster)  # clustering algorithms
library(factoextra) # visualization
# Load and preprocess the dataset
df<- iris[, 1:4]
df<- na.omit(df)
df<- scale(df)
# Dissimilarity matrix/Distance Matrix (describes pairwise distinction between M objects)
d <- dist(df, method="euclidean")
# Hierarchical clustering using Complete Linkage
hc <- hclust(d, method = "complete")
# Plot the obtained dendrogram
plot(hc, cex = 0.6, hang=-1)
sub_grps<- cutree(hc, k=3)
fviz_cluster(list(data = df, cluster = sub_grps))
plot(hc, cex = 0.6, hang=-1)
# Draw rectangle around HCluster
rect.hclust(hc, k = 3, border=2:4)
Simple Linear Regression
#Simple Linear Regression
# Importing the dataset
dataset =read.csv('salary.csv')
#creating the data containing 500 random values
data <- rnorm(500)
print(data)
#adding 10 random outliers to this data.
data[1:10] <- c(46,9,15,-90,42,50,-82,74,61,-32)
#draw boxpolot and an outlier is defined as a data point that is located outside the whiskers of the box plot.
boxplot(data)
#remove the outlier of the provided data boxplot.stats() function in R
data <- data[!data %in% boxplot.stats(data)$out]
#draw boxplot to verify whether ouliers removed or not
boxplot(data)
#creating the data containing 500 random values
data <- rnorm(500)
print(data)
#adding 10 random outliers to this data.
data[1:10] <- c(46,9,15,-90,42,50,-82,74,61,-32)
#draw boxpolot and an outlier is defined as a data point that is located outside the whiskers of the box plot.
boxplot(data)
#remove the outlier of the provided data boxplot.stats() function in R
data <- data[!data %in% boxplot.stats(data)$out]
#draw boxplot to verify whether ouliers removed or not
boxplot(data)
#creating the data containing 500 random values
data <- rnorm(500)
print(data)
#adding 10 random outliers to this data.
data[1:10] <- c(46,9,15,-90,42,50,-82,74,61,-32)
data[1:10] <- c(46,9,15,-90,42,50,-82,74,61,-32)
print(data)
#creating the data containing 500 random values
data <- rnorm(500)
print(data)
#adding 10 random outliers to this data.
data[1:10] <- c(46,9,15,-90,42,50,-82,74,61,-32)
#draw boxpolot and an outlier is defined as a data point that is located outside the whiskers of the box plot.
boxplot(data)
#remove the outlier of the provided data boxplot.stats() function in R
data <- data[!data %in% boxplot.stats(data)$out]
#draw boxplot to verify whether ouliers removed or not
boxplot(data)
#Aim:- Study of regression analysis using R Programming
# Generate random IQ values with mean = 30 and sd =2
IQ <- rnorm(40, 30, 2)
# Sorting IQ level in ascending order
IQ <- sort(IQ)
IQ
# Generate vector with pass and fail values of 40 students
result <- c(0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
1, 1, 1, 0, 1, 1, 1, 1, 0, 1)
# Data Frame
df <- as.data.frame(cbind(IQ, result))
print(df)
# Plotting IQ on x-axis and result on y-axis
plot(IQ, result, xlab = "IQ Level",ylab = "Probability of Passing")
# Create a Linear regression model
lrm <- lm(result ~ IQ)
summary(lrm)
# Create a logistic regression model #glm(Generalized Linear Models)
g = glm(result~IQ, family=binomial, df)
summary(g)
# Create a curve based on prediction using the regression model
curve(predict(g, data.frame(IQ=x), type="resp"), add=TRUE)
?data[!data %in% boxplot.stats(data)$out
out]
